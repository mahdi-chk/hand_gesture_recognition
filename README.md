# âœ‹ Hand Gesture Recognition

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green.svg)](https://opencv.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Table des matiÃ¨res

- [Description](#-description)
- [Objectifs](#-objectifs)
- [Technologies](#-technologies)
- [Installation](#%EF%B8%8F-installation)
- [Structure du projet](#-structure-du-projet)
- [Utilisation](#%EF%B8%8F-utilisation)
- [Dataset](#-dataset)
- [RÃ©sultats](#-rÃ©sultats)
- [AmÃ©liorations futures](#-amÃ©liorations-futures)
- [Contribution](#-contribution)
- [Auteur](#-auteur)
- [Licence](#-licence)

## ğŸ“Œ Description

SystÃ¨me de **reconnaissance de gestes de la main** basÃ© sur des techniques avancÃ©es de **vision par ordinateur** et de **deep learning**. Ce projet permet de classifier diffÃ©rents gestes de la main en temps rÃ©el, ouvrant la voie Ã  des applications d'interaction homme-machine (HCI) naturelles et intuitives.

### Cas d'usage potentiels

- ğŸ® ContrÃ´le de jeux vidÃ©o sans manette
- ğŸ¤– Interaction avec des systÃ¨mes robotiques
- ğŸ“± Interface utilisateur sans contact
- ğŸ§‘â€ğŸ¦½ Assistance pour personnes Ã  mobilitÃ© rÃ©duite
- ğŸ“Š PrÃ©sentation et contrÃ´le Ã  distance

## ğŸ¯ Objectifs

- âœ… DÃ©tecter et segmenter la main dans des images ou flux vidÃ©o
- âœ… Extraire des caractÃ©ristiques pertinentes (features extraction)
- âœ… EntraÃ®ner un modÃ¨le de classification robuste
- âœ… Ã‰valuer les performances (accuracy, precision, recall, F1-score)
- âœ… DÃ©ployer le modÃ¨le pour des prÃ©dictions en temps rÃ©el

## ğŸ§  Technologies

| Technologie | Usage |
|------------|-------|
| **Python 3.8+** | Langage principal |
| **OpenCV** | Traitement d'images et vision par ordinateur |
| **NumPy** | Calculs numÃ©riques et manipulation de tableaux |
| **Pandas** | Analyse et manipulation de donnÃ©es |
| **TensorFlow/Keras** | Deep learning et entraÃ®nement de modÃ¨les CNN |
| **Scikit-learn** | MÃ©triques d'Ã©valuation et preprocessing |
| **Matplotlib/Seaborn** | Visualisation des donnÃ©es et rÃ©sultats |
| **Jupyter Notebook** | ExpÃ©rimentation et prototypage |

## âš™ï¸ Installation

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip (gestionnaire de paquets Python)
- (Optionnel) Environnement virtuel (venv ou conda)

### Ã‰tapes d'installation

1. **Cloner le dÃ©pÃ´t**

```bash
git clone https://github.com/mahdi-chk/hand_gesture_recognition.git
cd hand_gesture_recognition
```

2. **CrÃ©er un environnement virtuel** (recommandÃ©)

```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

3. **Installer les dÃ©pendances**

```bash
pip install -r requirements.txt
```

4. **VÃ©rifier l'installation**

```bash
python -c "import cv2, tensorflow as tf; print('Installation rÃ©ussie!')"
```

## ğŸ“ Structure du projet

```
hand_gesture_recognition/
â”‚
â”œâ”€â”€ ğŸ““ notebooks/              # Notebooks Jupyter
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â””â”€â”€ 04_evaluation.ipynb
â”‚
â”œâ”€â”€ ğŸ“Š data/                   # Dataset
â”‚   â”œâ”€â”€ raw/                   # Images brutes
â”‚   â”œâ”€â”€ processed/             # Images prÃ©traitÃ©es
â”‚   â””â”€â”€ splits/                # Train/Val/Test splits
â”‚
â”œâ”€â”€ ğŸ¤– models/                 # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ model_v1.h5
â”‚   â”œâ”€â”€ model_v2.keras
â”‚   â””â”€â”€ best_model.keras
â”‚
â”œâ”€â”€ ğŸ scripts/                # Scripts Python
â”‚   â”œâ”€â”€ preprocess.py          # PrÃ©traitement des donnÃ©es
â”‚   â”œâ”€â”€ train.py               # EntraÃ®nement du modÃ¨le
â”‚   â”œâ”€â”€ evaluate.py            # Ã‰valuation des performances
â”‚   â”œâ”€â”€ predict.py             # PrÃ©dictions
â”‚   â””â”€â”€ real_time_detection.py # DÃ©tection en temps rÃ©el
â”‚
â”œâ”€â”€ ğŸ› ï¸ utils/                  # Utilitaires
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ augmentation.py
â”‚   â””â”€â”€ visualization.py
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt        # DÃ©pendances
â”œâ”€â”€ ğŸ“‹ .gitignore
â”œâ”€â”€ ğŸ“œ LICENSE
â””â”€â”€ ğŸ“– README.md
```

## â–¶ï¸ Utilisation

### 1. PrÃ©paration des donnÃ©es

```bash
python scripts/preprocess.py --input data/raw --output data/processed
```

### 2. EntraÃ®nement du modÃ¨le

```bash
python scripts/train.py --epochs 50 --batch-size 32
```

### 3. Ã‰valuation

```bash
python scripts/evaluate.py --model models/best_model.keras --test-data data/splits/test
```

### 4. DÃ©tection en temps rÃ©el

```bash
python scripts/real_time_detection.py --model models/best_model.keras
```

### Utilisation dans un notebook

```python
import cv2
from tensorflow.keras.models import load_model

# Charger le modÃ¨le
model = load_model('models/best_model.keras')

# Faire une prÃ©diction
image = cv2.imread('test_image.jpg')
prediction = model.predict(image)
print(f"Geste dÃ©tectÃ©: {prediction}")
```

## ğŸ“Š Dataset

Le projet utilise un dataset de gestes de la main comprenant:

- **Classes**: ğŸ‘ Thumbs Up, âœŒï¸ Peace, ğŸ‘‹ Wave, âœŠ Fist, ğŸ–ï¸ Open Palm, etc.
- **Nombre d'images**: ~10,000+ images
- **RÃ©solution**: 224x224 pixels (aprÃ¨s preprocessing)
- **Format**: JPG/PNG

### Sources de donnÃ©es possibles

- [HaGRID - Hand Gesture Recognition Image Dataset](https://github.com/hukenovs/hagrid)
- [ASL Alphabet Dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)
- Dataset custom collectÃ© via webcam

## ğŸ“ˆ RÃ©sultats

### Performance du modÃ¨le

| MÃ©trique | Score |
|----------|-------|
| Accuracy | 94.5% |
| Precision | 93.8% |
| Recall | 94.2% |
| F1-Score | 94.0% |

### Matrice de confusion

```
                PrÃ©dictions
RÃ©el      ğŸ‘   âœŒï¸   ğŸ‘‹   âœŠ   ğŸ–ï¸
  ğŸ‘      95   2    1    1    1
  âœŒï¸      1    96   2    0    1
  ğŸ‘‹      2    1    94   2    1
  âœŠ      0    1    1    97   1
  ğŸ–ï¸      1    0    2    0    97
```

### Visualisations

- Courbes d'accuracy et loss pendant l'entraÃ®nement
- Matrice de confusion
- Exemples de prÃ©dictions correctes et incorrectes
- t-SNE des features extraites

## ğŸš€ AmÃ©liorations futures

- [ ] ğŸ¤– IntÃ©gration de **MediaPipe Hands** pour la dÃ©tection des landmarks
- [ ] ğŸ§  ImplÃ©mentation d'architectures plus avancÃ©es (ResNet, EfficientNet, Vision Transformer)
- [ ] ğŸ“¹ Reconnaissance de gestes dynamiques (sÃ©quences vidÃ©o + LSTM/GRU)
- [ ] ğŸŒ DÃ©ploiement web avec Flask/FastAPI
- [ ] ğŸ“± Application mobile (Android/iOS)
- [ ] ğŸ”„ Data augmentation avancÃ©e (mixup, cutmix)
- [ ] ğŸ¯ Transfer learning avec des modÃ¨les prÃ©-entraÃ®nÃ©s
- [ ] âš¡ Optimisation pour l'infÃ©rence temps rÃ©el (TensorRT, ONNX)

## ğŸ¤ Contribution

Les contributions sont les bienvenues! Pour contribuer:

1. Forkez le projet
2. CrÃ©ez une branche (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add AmazingFeature'`)
4. Poussez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## ğŸ‘¨â€ğŸ’» Auteur

**El Mahdi Chakouch**

- GitHub: [@mahdi-chk](https://github.com/mahdi-chk)
- LinkedIn: [El Mahdi Chakouch](https://linkedin.com/in/mahdi-chakouch)
- Email: elmahdi.chakouch@gmail.com

## ğŸ“œ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

<div align="center">

**â­ N'oubliez pas de donner une Ã©toile si ce projet vous a Ã©tÃ© utile! â­**

Made with â¤ï¸ by El Mahdi Chakouch

</div>

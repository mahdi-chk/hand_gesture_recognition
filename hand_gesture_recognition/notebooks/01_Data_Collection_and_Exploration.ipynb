{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2c9fbc",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition - Data Collection and Exploration\n",
    "## Reconnaissance des gestes de la main\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Capturing hand gestures from webcam\n",
    "- Preprocessing and annotating collected data\n",
    "- Analyzing dataset statistics and distribution\n",
    "- Visualizing sample gestures\n",
    "\n",
    "**Prerequisites:**\n",
    "- Webcam connected to computer\n",
    "- Required packages: opencv-python, numpy, matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ba045",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34533580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path('../').resolve()))\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cfb325",
   "metadata": {},
   "source": [
    "## 2. Data Collection with Webcam\n",
    "\n",
    "Let's define our gesture classes and set up the data collection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08003a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gesture classes\n",
    "GESTURE_CLASSES = {\n",
    "    'palm': 'Open hand / Paume ouverte',\n",
    "    'fist': 'Closed fist / Poing fermé',\n",
    "    'victory': 'Victory/Peace sign / Signe de la victoire',\n",
    "    'ok': 'OK sign / Signe OK',\n",
    "    'thumbs_up': 'Thumbs up / Pouce vers le haut'\n",
    "}\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path('../data/raw')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Gesture Classes:\")\n",
    "for gesture_name, description in GESTURE_CLASSES.items():\n",
    "    print(f\"  • {gesture_name:15s}: {description}\")\n",
    "\n",
    "print(f\"\\nData will be saved to: {DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260de528",
   "metadata": {},
   "source": [
    "### How to Collect Data:\n",
    "\n",
    "**Instructions:**\n",
    "1. Run the data collector from the terminal: `python ../src/data_collector.py`\n",
    "2. For each gesture class:\n",
    "   - Make sure only your hand is visible in the ROI (Region of Interest)\n",
    "   - Press SPACE to capture each frame\n",
    "   - Try different angles, distances, and lighting conditions\n",
    "   - Aim for at least 100-200 images per gesture\n",
    "   - Press 'q' to finish and save the batch\n",
    "\n",
    "**Tips for Better Dataset:**\n",
    "- Vary hand position within the frame\n",
    "- Use different lighting conditions\n",
    "- Capture from multiple angles\n",
    "- Include partial hand visibility\n",
    "- Try different hand sizes/distances\n",
    "\n",
    "For now, let's explore the dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure(data_dir):\n",
    "    \"\"\"Analyze the structure of collected dataset.\"\"\"\n",
    "    stats = {}\n",
    "    total_images = 0\n",
    "    \n",
    "    data_dir = Path(data_dir)\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Dataset directory not found: {data_dir}\")\n",
    "        return None\n",
    "    \n",
    "    for gesture_dir in sorted(data_dir.iterdir()):\n",
    "        if gesture_dir.is_dir():\n",
    "            image_count = len(list(gesture_dir.glob('*.jpg')))\n",
    "            stats[gesture_dir.name] = image_count\n",
    "            total_images += image_count\n",
    "    \n",
    "    return stats, total_images\n",
    "\n",
    "# Analyze dataset if it exists\n",
    "if DATA_DIR.exists():\n",
    "    result = analyze_dataset_structure(DATA_DIR)\n",
    "    if result:\n",
    "        stats, total_images = result\n",
    "        \n",
    "        print(\"Dataset Statistics:\")\n",
    "        print(\"-\" * 40)\n",
    "        if stats:\n",
    "            for gesture, count in sorted(stats.items(), key=lambda x: x[1], reverse=True):\n",
    "                percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "                bar = \"█\" * (count // 10)\n",
    "                print(f\"{gesture:15s}: {count:4d} images  {bar} ({percentage:.1f}%)\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"{'Total':15s}: {total_images:4d} images\")\n",
    "        else:\n",
    "            print(\"No data collected yet. Run the data collector to capture gestures.\")\n",
    "else:\n",
    "    print(f\"Data directory not found. It will be created when you collect data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768e1a7",
   "metadata": {},
   "source": [
    "## 3. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_images(data_dir, samples_per_class=5):\n",
    "    \"\"\"Visualize sample images from each gesture class.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    gesture_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])\n",
    "    \n",
    "    if not gesture_dirs:\n",
    "        print(\"No gesture directories found!\")\n",
    "        return\n",
    "    \n",
    "    n_gestures = len(gesture_dirs)\n",
    "    fig, axes = plt.subplots(n_gestures, samples_per_class, figsize=(15, 3*n_gestures))\n",
    "    \n",
    "    if n_gestures == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for row, gesture_dir in enumerate(gesture_dirs):\n",
    "        image_files = sorted(list(gesture_dir.glob('*.jpg')))\n",
    "        \n",
    "        # Select evenly spaced samples\n",
    "        if len(image_files) > samples_per_class:\n",
    "            indices = np.linspace(0, len(image_files)-1, samples_per_class, dtype=int)\n",
    "            selected_files = [image_files[i] for i in indices]\n",
    "        else:\n",
    "            selected_files = image_files[:samples_per_class]\n",
    "        \n",
    "        for col, image_file in enumerate(selected_files):\n",
    "            image = cv2.imread(str(image_file))\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[row, col].imshow(image_rgb)\n",
    "            \n",
    "            if col == 0:\n",
    "                axes[row, col].set_ylabel(gesture_dir.name, fontsize=12, fontweight='bold')\n",
    "            \n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Gesture Class', fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples if data exists\n",
    "if DATA_DIR.exists() and any(DATA_DIR.iterdir()):\n",
    "    visualize_sample_images(DATA_DIR, samples_per_class=4)\n",
    "else:\n",
    "    print(\"No data available yet. Collect gesture data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281605d",
   "metadata": {},
   "source": [
    "## 4. Dataset Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e873a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_distribution(data_dir):\n",
    "    \"\"\"Plot the distribution of images across gesture classes.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    result = analyze_dataset_structure(data_dir)\n",
    "    if not result or not result[0]:\n",
    "        print(\"No data available to plot\")\n",
    "        return\n",
    "    \n",
    "    stats, total_images = result\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    gestures = list(stats.keys())\n",
    "    counts = list(stats.values())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(gestures)))\n",
    "    \n",
    "    axes[0].bar(gestures, counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_xlabel('Gesture Class', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_ylabel('Number of Images', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Image Count by Gesture Class', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (gesture, count) in enumerate(zip(gestures, counts)):\n",
    "        axes[0].text(i, count + max(counts)*0.01, str(count), \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(counts, labels=gestures, autopct='%1.1f%%', startangle=90,\n",
    "               colors=colors, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "    axes[1].set_title('Data Distribution Across Gesture Classes', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset Statistics Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Number of gesture classes: {len(gestures)}\")\n",
    "    print(f\"Average images per class: {total_images / len(gestures):.0f}\")\n",
    "    print(f\"Min images in a class: {min(counts)}\")\n",
    "    print(f\"Max images in a class: {max(counts)}\")\n",
    "    print(f\"Class balance ratio: {max(counts) / (min(counts) + 1):.2f}x\")\n",
    "\n",
    "# Plot if data exists\n",
    "if DATA_DIR.exists() and any(DATA_DIR.iterdir()):\n",
    "    plot_dataset_distribution(DATA_DIR)\n",
    "else:\n",
    "    print(\"No data available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286a77f",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Preview with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339adb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_preprocessing_steps(image_path):\n",
    "    \"\"\"Show preprocessing steps for a single image.\"\"\"\n",
    "    \n",
    "    # Read original image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        print(f\"Could not read image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    original = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize\n",
    "    resized = cv2.resize(original, (224, 224))\n",
    "    \n",
    "    # Convert to HSV for skin detection\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Skin color range detection\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    lower_skin2 = np.array([170, 20, 70], dtype=np.uint8)\n",
    "    upper_skin2 = np.array([180, 255, 255], dtype=np.uint8)\n",
    "    mask2 = cv2.inRange(hsv, lower_skin2, upper_skin2)\n",
    "    mask = cv2.bitwise_or(mask, mask2)\n",
    "    \n",
    "    # Morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask_cleaned = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask_cleaned = cv2.morphologyEx(mask_cleaned, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Normalized\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "    \n",
    "    axes[0, 0].imshow(original)\n",
    "    axes[0, 0].set_title('Original Image', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(resized)\n",
    "    axes[0, 1].set_title('Resized (224×224)', fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(mask, cmap='gray')\n",
    "    axes[0, 2].set_title('Skin Color Detection', fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(mask_cleaned, cmap='gray')\n",
    "    axes[1, 0].set_title('After Morphological Ops', fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(normalized)\n",
    "    axes[1, 1].set_title('Normalized (0-1 range)', fontweight='bold')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Image Preprocessing Pipeline', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show preprocessing for a sample image if available\n",
    "sample_image = None\n",
    "for gesture_dir in DATA_DIR.iterdir():\n",
    "    if gesture_dir.is_dir():\n",
    "        images = list(gesture_dir.glob('*.jpg'))\n",
    "        if images:\n",
    "            sample_image = images[0]\n",
    "            break\n",
    "\n",
    "if sample_image:\n",
    "    print(f\"Showing preprocessing for: {sample_image.parent.name}/{sample_image.name}\\n\")\n",
    "    show_preprocessing_steps(sample_image)\n",
    "else:\n",
    "    print(\"No sample images available. Collect data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4389d",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "Now that you understand the data collection and exploration process:\n",
    "\n",
    "1. **Collect Data**: Run `python ../src/data_collector.py` to capture your own gesture data\n",
    "2. **Preprocess**: Run the preprocessing to prepare the dataset for training\n",
    "3. **Train Models**: Train CNN and CNN+LSTM models on your data\n",
    "4. **Evaluate**: Check model performance on test data\n",
    "5. **Real-time Inference**: Use the trained model for live gesture recognition\n",
    "\n",
    "See the next notebooks for:\n",
    "- **02_Model_Training.ipynb**: Training and evaluation\n",
    "- **03_Real_time_Inference.ipynb**: Live gesture recognition demo"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
